{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import firebase_admin\n",
    "from firebase_admin import credentials , storage\n",
    "from firebase_admin import db\n",
    "import pandas as pd \n",
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "from supabase import create_client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Firebase Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = os.getenv('SUPABASE_URL')\n",
    "# key = os.getenv('SUPABASE_KEY')\n",
    "# supabase = create_client(url, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_account_info ={\n",
    "    \"type\": os.getenv('FIREBASE_TYPE'),\n",
    "    \"project_id\": os.getenv('FIREBASE_PROJECT_ID'),\n",
    "    \"private_key_id\": os.getenv('FIREBASE_PRIVATE_KEY_ID'),\n",
    "    \"private_key\": os.getenv('FIREBASE_PRIVATE_KEY'),\n",
    "    \"client_email\": os.getenv('FIREBASE_CLIENT_EMAIL'),\n",
    "    \"client_id\": os.getenv('FIREBASE_CLIENT_ID'),\n",
    "    \"auth_uri\": os.getenv('FIREBASE_AUTH_URI'),\n",
    "    \"token_uri\": os.getenv('FIREBASE_TOKEN_URI'),\n",
    "    \"auth_provider_x509_cert_url\": os.getenv('FIREBASE_AUTH_PROVIDER_X509_CERT_URL'),\n",
    "    \"client_x509_cert_url\": os.getenv('FIREBASE_CLIENT_X509_CERT_URL'),\n",
    "    \"universe_domain\": os.getenv('FIREBASE_UNIVERSE_DOMAIN')\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cred = credentials.Certificate(service_account_info)\n",
    "firebase_admin.initialize_app(cred,{\n",
    "    'storageBucket': 'coffee-shop-app-a4152.appspot.com',\n",
    "     'databaseURL': 'https://coffee-shop-d6777-default-rtdb.firebaseio.com/'\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_folder_path = './products/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>category</th>\n",
       "      <th>description</th>\n",
       "      <th>ingredients</th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>image_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cappuccino</td>\n",
       "      <td>Coffee</td>\n",
       "      <td>A rich and creamy cappuccino made with freshly...</td>\n",
       "      <td>[Espresso, Steamed Milk, Milk Foam]</td>\n",
       "      <td>4.50</td>\n",
       "      <td>4.7</td>\n",
       "      <td>cappuccino.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jumbo Savory Scone</td>\n",
       "      <td>Bakery</td>\n",
       "      <td>Deliciously flaky and buttery, this jumbo savo...</td>\n",
       "      <td>[Flour, Butter, Cheese, Herbs, Baking Powder, ...</td>\n",
       "      <td>3.25</td>\n",
       "      <td>4.3</td>\n",
       "      <td>SavoryScone.webp</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 name category  \\\n",
       "0          Cappuccino   Coffee   \n",
       "1  Jumbo Savory Scone   Bakery   \n",
       "\n",
       "                                         description  \\\n",
       "0  A rich and creamy cappuccino made with freshly...   \n",
       "1  Deliciously flaky and buttery, this jumbo savo...   \n",
       "\n",
       "                                         ingredients  price  rating  \\\n",
       "0                [Espresso, Steamed Milk, Milk Foam]   4.50     4.7   \n",
       "1  [Flour, Butter, Cheese, Herbs, Baking Powder, ...   3.25     4.3   \n",
       "\n",
       "         image_path  \n",
       "0    cappuccino.jpg  \n",
       "1  SavoryScone.webp  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('products/products.jsonl', lines=True)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_image(bucket_name, image_path):\n",
    "    image_name = image_path.split('/')[-1]\n",
    "    file_path_in_bucket = f'product_images/{image_name}'\n",
    "    \n",
    "    # Check if the file already exists\n",
    "    existing_file = supabase.storage.from_(bucket_name).list(path='product_images/', search=image_name)\n",
    "    if existing_file:\n",
    "        print(f\"{image_name} already exists. Skipping upload.\")\n",
    "    else:\n",
    "        with open(image_path, \"rb\") as f:\n",
    "            response = supabase.storage.from_(bucket_name).upload(file_path_in_bucket, f)\n",
    "    \n",
    "    # Generate public URL\n",
    "    public_url = f\"{url}/storage/v1/object/public/{bucket_name}/{file_path_in_bucket}\"\n",
    "    return public_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Cappuccino\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "SyncBucketActionsMixin.list() got an unexpected keyword argument 'search'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(index, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m image_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(image_folder_path, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 5\u001b[0m image_url \u001b[38;5;241m=\u001b[39m upload_image(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproducts\u001b[39m\u001b[38;5;124m'\u001b[39m, image_path)  \u001b[38;5;66;03m# 'products' is the bucket name\u001b[39;00m\n\u001b[1;32m      7\u001b[0m product_data \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mto_dict()\n\u001b[1;32m      8\u001b[0m product_data\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Remove image_path from the data\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[34], line 6\u001b[0m, in \u001b[0;36mupload_image\u001b[0;34m(bucket_name, image_path)\u001b[0m\n\u001b[1;32m      3\u001b[0m file_path_in_bucket \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_images/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Check if the file already exists\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m existing_file \u001b[38;5;241m=\u001b[39m supabase\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mfrom_(bucket_name)\u001b[38;5;241m.\u001b[39mlist(path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_images/\u001b[39m\u001b[38;5;124m'\u001b[39m, search\u001b[38;5;241m=\u001b[39mimage_name)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m existing_file:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m already exists. Skipping upload.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: SyncBucketActionsMixin.list() got an unexpected keyword argument 'search'"
     ]
    }
   ],
   "source": [
    "for index, row in df.iterrows():\n",
    "    print(index, row['name'])\n",
    "    \n",
    "    image_path = os.path.join(image_folder_path, row['image_path'])\n",
    "    image_url = upload_image('products', image_path)  # 'products' is the bucket name\n",
    "    \n",
    "    product_data = row.to_dict()\n",
    "    product_data.pop('image_path')  # Remove image_path from the data\n",
    "    product_data['image_url'] = image_url  # Add image_url for the product\n",
    "    \n",
    "    # Ensure product_data matches the table schema\n",
    "    try:\n",
    "        # Insert data into Supabase table 'products'\n",
    "        response = supabase.table('products').insert(product_data).execute()\n",
    "        if response.status_code == 201:\n",
    "            print(f\"Product {row['name']} uploaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Error uploading {row['name']}: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting product {row['name']}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 name category  \\\n",
      "0          Cappuccino   Coffee   \n",
      "1  Jumbo Savory Scone   Bakery   \n",
      "\n",
      "                                         description  \\\n",
      "0  A rich and creamy cappuccino made with freshly...   \n",
      "1  Deliciously flaky and buttery, this jumbo savo...   \n",
      "\n",
      "                                         ingredients  price  rating  \\\n",
      "0                [Espresso, Steamed Milk, Milk Foam]   4.50     4.7   \n",
      "1  [Flour, Butter, Cheese, Herbs, Baking Powder, ...   3.25     4.3   \n",
      "\n",
      "         image_path  \n",
      "0    cappuccino.jpg  \n",
      "1  SavoryScone.webp  \n",
      "0 Cappuccino\n",
      "Image 'cappuccino.jpg' uploaded successfully with ID: 679e0c3c60d1ee3f289327fd\n",
      "Product Cappuccino uploaded successfully with ID: 679e0c3d60d1ee3f289327ff\n",
      "1 Jumbo Savory Scone\n",
      "Image 'SavoryScone.webp' uploaded successfully with ID: 679e0c3d60d1ee3f28932800\n",
      "Product Jumbo Savory Scone uploaded successfully with ID: 679e0c3e60d1ee3f28932805\n",
      "2 Latte\n",
      "Image 'Latte.jpg' uploaded successfully with ID: 679e0c3e60d1ee3f28932806\n",
      "Product Latte uploaded successfully with ID: 679e0c3e60d1ee3f2893280a\n",
      "3 Chocolate Chip Biscotti\n",
      "Image 'chocolat_biscotti.jpg' uploaded successfully with ID: 679e0c3e60d1ee3f2893280b\n",
      "Product Chocolate Chip Biscotti uploaded successfully with ID: 679e0c3f60d1ee3f2893280d\n",
      "4 Espresso shot\n",
      "Image 'Espresso_shot.webp' uploaded successfully with ID: 679e0c3f60d1ee3f2893280e\n",
      "Product Espresso shot uploaded successfully with ID: 679e0c3f60d1ee3f28932810\n",
      "5 Hazelnut Biscotti\n",
      "Image 'Hazelnut_Biscotti.jpg' uploaded successfully with ID: 679e0c3f60d1ee3f28932811\n",
      "Product Hazelnut Biscotti uploaded successfully with ID: 679e0c3f60d1ee3f28932813\n",
      "6 Chocolate Croissant\n",
      "Image 'Chocolate_Croissant.jpg' uploaded successfully with ID: 679e0c3f60d1ee3f28932814\n",
      "Product Chocolate Croissant uploaded successfully with ID: 679e0c3f60d1ee3f28932817\n",
      "7 Dark chocolate\n",
      "Image 'Dark_chocolate.jpg' uploaded successfully with ID: 679e0c3f60d1ee3f28932818\n",
      "Product Dark chocolate uploaded successfully with ID: 679e0c4060d1ee3f2893281a\n",
      "8 Cranberry Scone\n",
      "Image 'Cranberry_Scone.jpg' uploaded successfully with ID: 679e0c4060d1ee3f2893281b\n",
      "Product Cranberry Scone uploaded successfully with ID: 679e0c4060d1ee3f2893281d\n",
      "9 Croissant\n",
      "Image 'Croissant.jpg' uploaded successfully with ID: 679e0c4060d1ee3f2893281e\n",
      "Product Croissant uploaded successfully with ID: 679e0c4060d1ee3f28932820\n",
      "10 Almond Croissant\n",
      "Image 'almond_croissant.jpg' uploaded successfully with ID: 679e0c4060d1ee3f28932821\n",
      "Product Almond Croissant uploaded successfully with ID: 679e0c4060d1ee3f28932824\n",
      "11 Ginger Biscotti\n",
      "Image 'Ginger_Biscotti.webp' uploaded successfully with ID: 679e0c4060d1ee3f28932825\n",
      "Product Ginger Biscotti uploaded successfully with ID: 679e0c4160d1ee3f28932827\n",
      "12 Oatmeal Scone\n",
      "Image 'oatmeal_scones.jpg' uploaded successfully with ID: 679e0c4160d1ee3f28932828\n",
      "Product Oatmeal Scone uploaded successfully with ID: 679e0c4160d1ee3f2893282a\n",
      "13 Ginger Scone\n",
      "Image 'Ginger_Scone.webp' uploaded successfully with ID: 679e0c4160d1ee3f2893282b\n",
      "Product Ginger Scone uploaded successfully with ID: 679e0c4160d1ee3f2893282d\n",
      "14 Chocolate syrup\n",
      "Image 'Chocolate_syrup.jpg' uploaded successfully with ID: 679e0c4160d1ee3f2893282e\n",
      "Product Chocolate syrup uploaded successfully with ID: 679e0c4160d1ee3f28932831\n",
      "15 Hazelnut syrup\n",
      "Image 'Hazelnut_syrup.webp' uploaded successfully with ID: 679e0c4160d1ee3f28932832\n",
      "Product Hazelnut syrup uploaded successfully with ID: 679e0c4260d1ee3f28932834\n",
      "16 Carmel syrup\n",
      "Image 'caramel_syrup.jpg' uploaded successfully with ID: 679e0c4260d1ee3f28932835\n",
      "Product Carmel syrup uploaded successfully with ID: 679e0c4260d1ee3f28932837\n",
      "17 Sugar Free Vanilla syrup\n",
      "Image 'Vanilla_syrup.jpg' uploaded successfully with ID: 679e0c4260d1ee3f28932838\n",
      "Product Sugar Free Vanilla syrup uploaded successfully with ID: 679e0c4260d1ee3f2893283a\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "import gridfs\n",
    "import pandas as pd\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# MongoDB Atlas connection URI (Replace with your MongoDB Atlas URI)\n",
    "MONGO_URI = os.getenv('MONGO_URI')\n",
    "\n",
    "# Connect to MongoDB Atlas\n",
    "client = MongoClient(MONGO_URI)\n",
    "db = client['product_db']  # Database name\n",
    "fs = gridfs.GridFS(db)  # GridFS for handling large files\n",
    "\n",
    "# Folder paths\n",
    "image_folder_path = './products/images/'\n",
    "\n",
    "# Load product data\n",
    "df = pd.read_json('products/products.jsonl', lines=True)\n",
    "print(df.head(2))\n",
    "\n",
    "# Function to upload image to GridFS\n",
    "def upload_image(image_path):\n",
    "    image_name = image_path.split('/')[-1]\n",
    "    with open(image_path, \"rb\") as img_file:\n",
    "        # Upload the image to GridFS and get its file_id\n",
    "        file_id = fs.put(img_file, filename=image_name)\n",
    "        print(f\"Image '{image_name}' uploaded successfully with ID: {file_id}\")\n",
    "    return file_id\n",
    "\n",
    "# Function to generate public URL (you can serve this with a custom server or use MongoDB Data API)\n",
    "def generate_image_url(file_id):\n",
    "    # Since MongoDB itself doesn't provide a public URL, we assume you would use a custom server for serving the image\n",
    "    return f\"/image/{file_id}\"\n",
    "\n",
    "# Upload images and store product data in MongoDB\n",
    "for index, row in df.iterrows():\n",
    "    print(index, row['name'])\n",
    "    \n",
    "    # Image path\n",
    "    image_path = os.path.join(image_folder_path, row['image_path'])\n",
    "    \n",
    "    # Upload image to GridFS\n",
    "    file_id = upload_image(image_path)\n",
    "    \n",
    "    # Prepare product data for insertion into MongoDB\n",
    "    product_data = row.to_dict()\n",
    "    product_data.pop('image_path')  # Remove image_path from the data\n",
    "    product_data['image_file_id'] = file_id  # Store file_id from GridFS\n",
    "    product_data['image_url'] = generate_image_url(file_id)  # Generate image URL\n",
    "    \n",
    "    # Insert product data into the 'products' collection\n",
    "    try:\n",
    "        response = db.products.insert_one(product_data)\n",
    "        print(f\"Product {row['name']} uploaded successfully with ID: {response.inserted_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while inserting product {row['name']}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
